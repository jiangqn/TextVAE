seed: 42
base_path: data/yelp
glove_source_path: /home/jiangqn/datasets/embeddings/glove.840B.300d.txt
max_len: 40
features: ["label", "length", "depth"]
text_cnn:
  model:
    embed_size: 300
    kernel_sizes: [3, 4, 5]
    kernel_num: 100
    dropout: 0.5
  training:
    batch_size: 64
    lr: 0.001
    weight_decay: 0
    epoches: 5
    eval_freq: 100
    max_patience: 20
language_model:
  model:
    embed_size: 300
    hidden_size: 300
    num_layers: 1
    dropout: 0.5
    weight_tying: True
  training:
    batch_size: 64
    lr: 0.001
    weight_decay: 0.0001
    epoches: 5
    eval_freq: 100
    clip_grad_norm: 5.0
    max_patience: 20
text_vae:
  encoder:
    encoder_type: "transformer_encoder"
    bow_encoder:
      embed_size: 300
      dropout: 0.5
    bow_mlp_encoder:
      embed_size: 300
      hidden_size: 300
      dropout: 0.5
    conv_encoder:
      embed_size: 300
      kernel_sizes: [3, 4, 5]
      kernel_num: 100
      num_layers: 1
      dropout: 0.5
    gru_encoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 1
      bidirectional: True
      dropout: 0.5
      output_type: "attention_pooling"
    lstm_encoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 2
      bidirectional: True
      dropout: 0.5
      output_type: "attention_pooling"
    transformer_encoder:
      embed_size: 300
      hidden_size: 512
      ff_size: 1024
      num_layers: 3
      num_heads: 4
      dropout: 0.5
  decoder:
    decoder_type: "transformer_decoder"
    gru_decoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 1
      dropout: 0.5
      word_dropout: 0.5
      decoder_generator_tying: True
      initial_hidden_type: "zero"
    lstm_decoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 1
      dropout: 0.5
      word_dropout: 0.5
      decoder_generator_tying: True
      initial_hidden_type: "zero"
    skip_gru_decoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 1
      dropout: 0.5
      word_dropout: 0.5
      decoder_generator_tying: True
      initial_hidden_type: "zero"
    skip_lstm_decoder:
      embed_size: 300
      hidden_size: 600
      num_layers: 2
      dropout: 0.5
      word_dropout: 0.5
      decoder_generator_tying: True
      initial_hidden_type: "zero"
    transformer_decoder:
      embed_size: 300
      hidden_size: 512
      ff_size: 1024
      num_layers: 2
      num_heads: 4
      dropout: 0.5
      word_dropout: 0.5
      decoder_generator_tying: True
  latent_size: 100
  encoder_decoder_tying: True
  training:
    batch_size: 64
    beta: 0.8
    anneal:
      type: "linear"
      step: 10000
      offset: 5
    lr: 0.0001
    weight_decay: 0
    epoches: 5
    eval_freq: 100
    clip_grad_norm: 5.0
sample:
  vanilla_sample:
    train_sample_num: 100000
    dev_sample_num: 20000
    test_sample_num: 100000
  categorical_sample:
    sample_num: 100000
    save_encoding: False
  length_sample:
    sample_num: 100000
    save_encoding: False
  depth_sample:
    sample_num: 100000
    save_encoding: False